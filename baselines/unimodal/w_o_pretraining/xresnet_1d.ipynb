{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Task-1 (mPAP)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1378aad5d65234f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Apr 26 14:34:18 2023\n",
    "Latest Change on Wed May 03 09:56:10 2023\n",
    "\n",
    "@author: hawkiyc\n",
    "\"\"\"\n",
    "\n",
    "'Import Libraries'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "'Set Activation Function'\n",
    "if __name__ == \"__main__\":\n",
    "    stem_k, block_k = 11, 5\n",
    "    activation = nn.ReLU(inplace=True)\n",
    "    data_dim = 12\n",
    "    out_dim = 5\n",
    "\n",
    "'Bulid th Model'\n",
    "\n",
    "\n",
    "class conv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, k_size=25, stride=1,\n",
    "                 drop_r=None, zero_batch_norm=False,\n",
    "                 bias=False, use_act_fun=True,\n",
    "                 act_fun: nn.Module = activation):\n",
    "\n",
    "        assert k_size % 2 == 1, 'kernel size shall be odd number'\n",
    "        super(conv, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_ch, out_ch, k_size, stride,\n",
    "                                padding=(k_size - 1) // 2, bias=bias, )\n",
    "        self.batch_norm = nn.BatchNorm1d(out_ch)\n",
    "        nn.init.constant_(self.batch_norm.weight,\n",
    "                          0. if zero_batch_norm else 1.)\n",
    "        self.act_fun = act_fun\n",
    "        self.drop_r, self.drop = drop_r, nn.Dropout(drop_r\n",
    "                                                    ) if drop_r else None\n",
    "        self.use_act_fun = use_act_fun\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1d(x)\n",
    "        x = self.batch_norm(x)\n",
    "        if self.use_act_fun:\n",
    "            x = self.act_fun(x)\n",
    "        if self.drop_r:\n",
    "            x = self.drop(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class XResNetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, expansion, in_ch, between_ch, k=9,\n",
    "                 stride=1, b_verbose=None,\n",
    "                 act_fun: nn.Module = activation):\n",
    "\n",
    "        assert expansion in [1, 4], 'expansion shall be 1 or 4'\n",
    "        super(XResNetBlock, self).__init__()\n",
    "\n",
    "        in_ch = in_ch * expansion\n",
    "        out_ch = between_ch * expansion\n",
    "\n",
    "        if expansion == 1:\n",
    "\n",
    "            layers = [conv(in_ch, between_ch,\n",
    "                           k, stride=stride),\n",
    "                      conv(between_ch, out_ch, k,\n",
    "                           zero_batch_norm=True,\n",
    "                           use_act_fun=False)]\n",
    "\n",
    "        else:\n",
    "\n",
    "            layers = [conv(in_ch, between_ch, 1),\n",
    "                      conv(between_ch, between_ch,\n",
    "                           k, stride=stride, ),\n",
    "                      conv(between_ch, out_ch, 1,\n",
    "                           zero_batch_norm=True,\n",
    "                           use_act_fun=False)]\n",
    "\n",
    "        self.xres_block = nn.ModuleList(layers)\n",
    "\n",
    "        self.res_conv = conv(in_ch, out_ch, 1, use_act_fun=False\n",
    "                             ) if in_ch != out_ch else None\n",
    "        self.res_pool = nn.AvgPool1d(2, ceil_mode=True\n",
    "                                     ) if stride != 1 else None\n",
    "        self.act_fun = act_fun\n",
    "        self.b_verbose = b_verbose if b_verbose else None\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        identity = x\n",
    "\n",
    "        for l in self.xres_block:\n",
    "            x = l(x)\n",
    "            print('res_torch_size:', x.shape) if self.b_verbose else None\n",
    "\n",
    "        identity = self.res_pool(identity) if self.res_pool else identity\n",
    "        identity = self.res_conv(identity) if self.res_conv else identity\n",
    "        print('identity_torch_size:', x.shape) if self.b_verbose else None\n",
    "\n",
    "        x += identity\n",
    "        x = self.act_fun(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConcatPool(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.maxpool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxpooled = self.maxpool(x).squeeze(self.dim)\n",
    "        avgpooled = self.avgpool(x).squeeze(self.dim)\n",
    "\n",
    "        return torch.cat((maxpooled, avgpooled), dim=self.dim)\n",
    "\n",
    "\n",
    "class XResNet1d(nn.Module):\n",
    "\n",
    "    def __init__(self, expansion, num_layers, stem_k,\n",
    "                 block_k, in_ch=data_dim, c_out=out_dim,\n",
    "                 model_drop_r=None, verbose=False,\n",
    "                 b_verbose=False, original_f_number=False,\n",
    "                 fc_drop=None):\n",
    "\n",
    "        super(XResNet1d, self).__init__()\n",
    "\n",
    "        stem_filters = [in_ch, 32, 32, 64]\n",
    "\n",
    "        stem = [conv(stem_filters[i], stem_filters[i + 1], k_size=stem_k,\n",
    "                     stride=2 if i == 0 else 1, drop_r=model_drop_r,\n",
    "                     ) for i in range(3)]\n",
    "        self.stem = nn.ModuleList(stem)\n",
    "\n",
    "        self.stem_pool = nn.MaxPool1d(3, 2, padding=1)\n",
    "        self.model_drop_r = nn.Dropout(model_drop_r\n",
    "                                       ) if model_drop_r else None\n",
    "        self.b_verbose = b_verbose if b_verbose else None\n",
    "\n",
    "        if original_f_number:\n",
    "\n",
    "            block_filters = [64 // expansion] + [(o) for o in [\n",
    "                64, 128, 256, 512] + [256] * (len(num_layers) - 4)]\n",
    "        else:\n",
    "\n",
    "            block_filters = [64 // expansion] + [(o) for o in [\n",
    "                64, 64, 64, 64] + [32] * (len(num_layers) - 4)]\n",
    "\n",
    "        self.block_k = block_k\n",
    "        block = [self.make_layers(expansion, block_filters[i],\n",
    "                                  block_filters[i + 1], n_blocks=l,\n",
    "                                  stride=1 if i == 0 else 2,\n",
    "                                  ) for i, l in enumerate(num_layers)]\n",
    "        self.block = nn.ModuleList(block)\n",
    "\n",
    "        self.concat_pool = ConcatPool()\n",
    "        self.fc1 = nn.Linear(block_filters[-1] * expansion * 2, 128)\n",
    "        self.fc_batch_norm = nn.BatchNorm1d(128)\n",
    "        self.fc_drop = nn.Dropout(fc_drop) if fc_drop else None\n",
    "        self.fc_out = nn.Linear(128, c_out)\n",
    "        self.expansion = expansion\n",
    "        self.verbose = verbose\n",
    "\n",
    "        for m in self.modules():\n",
    "\n",
    "            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "            if getattr(m, 'bias', None) is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def make_layers(self, expansion, n_inputs,\n",
    "                    n_filters, n_blocks, stride,\n",
    "                    ):\n",
    "\n",
    "        sub_block = []\n",
    "\n",
    "        if self.model_drop_r:\n",
    "\n",
    "            for i in range(n_blocks):\n",
    "                sub_block.append(XResNetBlock(expansion,\n",
    "                                              n_inputs if i == 0 else n_filters,\n",
    "                                              n_filters, self.block_k,\n",
    "                                              stride if i == 0 else 1,\n",
    "                                              b_verbose=self.b_verbose\n",
    "                                              if self.b_verbose else None,\n",
    "                                              ))\n",
    "                sub_block.append(self.model_drop_r)\n",
    "\n",
    "        else:\n",
    "            sub_block = [XResNetBlock(expansion,\n",
    "                                      n_inputs if i == 0 else n_filters,\n",
    "                                      n_filters, self.block_k,\n",
    "                                      stride if i == 0 else 1,\n",
    "                                      b_verbose=self.b_verbose\n",
    "                                      if self.b_verbose else None,\n",
    "                                      ) for i in range(n_blocks)]\n",
    "\n",
    "        return nn.Sequential(*sub_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        for l in self.stem:\n",
    "            x = l(x)\n",
    "            print('stem_torch_size:', x.shape) if self.verbose else None\n",
    "\n",
    "        x = self.stem_pool(x)\n",
    "\n",
    "        for b in self.block:\n",
    "            x = b(x)\n",
    "            print('block_torch_size:', x.shape) if self.verbose else None\n",
    "\n",
    "        x = self.concat_pool(x)\n",
    "        print('concat_pool_torch_size:', x.shape) if self.verbose else None\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc_batch_norm(x)\n",
    "        x = self.fc_drop(x) if self.fc_drop else x\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-01T19:47:35.943163600Z",
     "start_time": "2025-08-01T19:47:34.398293800Z"
    }
   },
   "id": "a8113363c8e1d435",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, matthews_corrcoef\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# **Set device configuration**\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# **Set seed for reproducibility**\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "# Define file paths for the 12-lead ECG data and labels\n",
    "lead_file_paths = {\n",
    "    \"LEAD_I\": \"../../Data_processing/data_aspire_PAP_1/LEAD_I.pt\",\n",
    "    \"LEAD_II\": \"../../Data_processing/data_aspire_PAP_1/LEAD_II.pt\",\n",
    "    \"LEAD_III\": \"../../Data_processing/data_aspire_PAP_1/LEAD_III.pt\",\n",
    "    \"LEAD_aVR\": \"../../Data_processing/data_aspire_PAP_1/LEAD_aVR.pt\",\n",
    "    \"LEAD_aVL\": \"../../Data_processing/data_aspire_PAP_1/LEAD_aVL.pt\",\n",
    "    \"LEAD_aVF\": \"../../Data_processing/data_aspire_PAP_1/LEAD_aVF.pt\",\n",
    "    \"LEAD_V1\": \"../../Data_processing/data_aspire_PAP_1/LEAD_V1.pt\",\n",
    "    \"LEAD_V2\": \"../../Data_processing/data_aspire_PAP_1/LEAD_V2.pt\",\n",
    "    \"LEAD_V3\": \"../../Data_processing/data_aspire_PAP_1/LEAD_V3.pt\",\n",
    "    \"LEAD_V4\": \"../../Data_processing/data_aspire_PAP_1/LEAD_V4.pt\",\n",
    "    \"LEAD_V5\": \"../../Data_processing/data_aspire_PAP_1/LEAD_V5.pt\",\n",
    "    \"LEAD_V6\": \"../../Data_processing/data_aspire_PAP_1/LEAD_V6.pt\"\n",
    "}\n",
    "labels_file_path = \"../../Data_processing/data_aspire_PAP_1/labels.pt\"\n",
    "\n",
    "# Load all lead tensors and labels\n",
    "ecg_lead_tensors = {lead: torch.load(path) for lead, path in lead_file_paths.items()}\n",
    "labels = torch.load(labels_file_path)\n",
    "\n",
    "# Ensure all leads have the same number of samples\n",
    "sample_count = len(next(iter(ecg_lead_tensors.values())))\n",
    "assert len(labels) == sample_count, \"Mismatch between number of labels and samples.\"\n",
    "for tensor in ecg_lead_tensors.values():\n",
    "    assert len(tensor) == sample_count, \"All leads must have the same number of samples.\"\n",
    "\n",
    "# **Dataset Class**\n",
    "class ECGMultiLeadDatasetWithLabels(Dataset):\n",
    "    def __init__(self, ecg_leads, labels, lead_names):\n",
    "        self.ecg_leads = {lead: ecg_leads[lead] for lead in lead_names}\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.ecg_leads.values())))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lead_data = {lead: self.ecg_leads[lead][idx].unsqueeze(0) for lead in self.ecg_leads}\n",
    "        label = self.labels[idx]\n",
    "        # Stack lead data to form a tensor of shape (num_leads, sequence_length)\n",
    "        lead_tensor = torch.cat([lead_data[lead] for lead in lead_data], dim=0)\n",
    "        return lead_tensor, label\n",
    "\n",
    "# Define the set of leads (choose between 6-lead and 12-lead configurations)\n",
    "use_6_leads = False  # Change to False for 12-lead ECG\n",
    "lead_names = (\n",
    "    [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVL\", \"LEAD_aVF\"]\n",
    "    if use_6_leads\n",
    "    else list(lead_file_paths.keys())\n",
    ")\n",
    "\n",
    "# **Initialize the dataset and dataloader**\n",
    "dataset = ECGMultiLeadDatasetWithLabels(ecg_lead_tensors, labels, lead_names)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "class ECGResNet1D(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes=2):\n",
    "        super(ECGResNet1D, self).__init__()\n",
    "\n",
    "        # Optimized configuration for 10-second ECG data at ~500 Hz\n",
    "        self.resnet = XResNet1d(\n",
    "            expansion=1,\n",
    "            num_layers=[3, 4, 6, 3],  # Optimized for longer ECG sequences\n",
    "            stem_k=11,                # For higher sampling rates\n",
    "            block_k=5,                # Moderate local feature extraction\n",
    "            in_ch=input_channels,\n",
    "            c_out=num_classes,\n",
    "            model_drop_r=0.07,         # Slightly higher dropout for regularization\n",
    "            verbose=False,\n",
    "            b_verbose=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "\n",
    "# **Training and Evaluation Functions**\n",
    "def train_classifier(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            lead_data, labels = batch\n",
    "            lead_data, labels = lead_data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(lead_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            lead_data, labels = batch\n",
    "            lead_data, labels = lead_data.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(lead_data)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, auc_score, f1, mcc\n",
    "\n",
    "# **Cross-Validation Training**\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subset = Subset(dataset, train_ids)\n",
    "    test_subset = Subset(dataset, test_ids)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize XResNet1d model\n",
    "    model = ECGResNet1D(input_channels=len(lead_names), num_classes=2).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate\n",
    "    train_classifier(model, train_loader, criterion, optimizer, epochs=50)\n",
    "    accuracy, auc_score, f1, mcc = evaluate_model(model, test_loader)\n",
    "    fold_results.append((accuracy, auc_score, f1, mcc))\n",
    "\n",
    "    print(f'Fold {fold} Results: Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}\\n')\n",
    "\n",
    "# **Calculate Metrics Across Folds**\n",
    "accuracies, aucs, f1s, mccs = zip(*fold_results)\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f}, STD: {np.std(accuracies):.4f}')\n",
    "print(f'Mean AUC: {np.mean(aucs):.4f}, STD: {np.std(aucs):.4f}')\n",
    "print(f'Mean F1: {np.mean(f1s):.4f}, STD: {np.std(f1s):.4f}')\n",
    "print(f'Mean MCC: {np.mean(mccs):.4f}, STD: {np.std(mccs):.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1049b0a752d3ca20",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Task-2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "387bf49c09b5c10a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, matthews_corrcoef\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# **Set device configuration**\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# **Set seed for reproducibility**\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "# Define file paths for the 12-lead ECG data and labels\n",
    "lead_file_paths = {\n",
    "    \"LEAD_I\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_I.pt\",\n",
    "    \"LEAD_II\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_II.pt\",\n",
    "    \"LEAD_III\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_III.pt\",\n",
    "    \"LEAD_aVR\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_aVR.pt\",\n",
    "    \"LEAD_aVL\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_aVL.pt\",\n",
    "    \"LEAD_aVF\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_aVF.pt\",\n",
    "    \"LEAD_V1\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_V1.pt\",\n",
    "    \"LEAD_V2\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_V2.pt\",\n",
    "    \"LEAD_V3\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_V3.pt\",\n",
    "    \"LEAD_V4\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_V4.pt\",\n",
    "    \"LEAD_V5\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_V5.pt\",\n",
    "    \"LEAD_V6\": \"../../Data_processing/data_aspire_PAWP_1/LEAD_V6.pt\"\n",
    "}\n",
    "labels_file_path = \"../../Data_processing/data_aspire_PAWP_1/labels.pt\"\n",
    "\n",
    "\n",
    "# Load all lead tensors and labels\n",
    "ecg_lead_tensors = {lead: torch.load(path) for lead, path in lead_file_paths.items()}\n",
    "labels = torch.load(labels_file_path)\n",
    "\n",
    "# Ensure all leads have the same number of samples\n",
    "sample_count = len(next(iter(ecg_lead_tensors.values())))\n",
    "assert len(labels) == sample_count, \"Mismatch between number of labels and samples.\"\n",
    "for tensor in ecg_lead_tensors.values():\n",
    "    assert len(tensor) == sample_count, \"All leads must have the same number of samples.\"\n",
    "\n",
    "# **Dataset Class**\n",
    "class ECGMultiLeadDatasetWithLabels(Dataset):\n",
    "    def __init__(self, ecg_leads, labels, lead_names):\n",
    "        self.ecg_leads = {lead: ecg_leads[lead] for lead in lead_names}\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.ecg_leads.values())))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lead_data = {lead: self.ecg_leads[lead][idx].unsqueeze(0) for lead in self.ecg_leads}\n",
    "        label = self.labels[idx]\n",
    "        # Stack lead data to form a tensor of shape (num_leads, sequence_length)\n",
    "        lead_tensor = torch.cat([lead_data[lead] for lead in lead_data], dim=0)\n",
    "        return lead_tensor, label\n",
    "\n",
    "# Define the set of leads (choose between 6-lead and 12-lead configurations)\n",
    "use_6_leads = False  # Change to False for 12-lead ECG\n",
    "lead_names = (\n",
    "    [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVL\", \"LEAD_aVF\"]\n",
    "    if use_6_leads\n",
    "    else list(lead_file_paths.keys())\n",
    ")\n",
    "\n",
    "# **Initialize the dataset and dataloader**\n",
    "dataset = ECGMultiLeadDatasetWithLabels(ecg_lead_tensors, labels, lead_names)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "class ECGResNet1D(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes=2):\n",
    "        super(ECGResNet1D, self).__init__()\n",
    "\n",
    "        # Optimized configuration for 10-second ECG data at ~500 Hz\n",
    "        self.resnet = XResNet1d(\n",
    "            expansion=1,\n",
    "            num_layers=[3, 4, 6, 3],  # Optimized for longer ECG sequences\n",
    "            stem_k=11,                # For higher sampling rates\n",
    "            block_k=5,                # Moderate local feature extraction\n",
    "            in_ch=input_channels,\n",
    "            c_out=num_classes,\n",
    "            model_drop_r=0.07,         # Slightly higher dropout for regularization\n",
    "            verbose=False,\n",
    "            b_verbose=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "# **Training and Evaluation Functions**\n",
    "def train_classifier(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            lead_data, labels = batch\n",
    "            lead_data, labels = lead_data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(lead_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            lead_data, labels = batch\n",
    "            lead_data, labels = lead_data.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(lead_data)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, auc_score, f1, mcc\n",
    "\n",
    "# **Cross-Validation Training**\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subset = Subset(dataset, train_ids)\n",
    "    test_subset = Subset(dataset, test_ids)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize XResNet1d model\n",
    "    model = ECGResNet1D(input_channels=len(lead_names), num_classes=2).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate\n",
    "    train_classifier(model, train_loader, criterion, optimizer, epochs=50)\n",
    "    accuracy, auc_score, f1, mcc = evaluate_model(model, test_loader)\n",
    "    fold_results.append((accuracy, auc_score, f1, mcc))\n",
    "\n",
    "    print(f'Fold {fold} Results: Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}\\n')\n",
    "\n",
    "# **Calculate Metrics Across Folds**\n",
    "accuracies, aucs, f1s, mccs = zip(*fold_results)\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f}, STD: {np.std(accuracies):.4f}')\n",
    "print(f'Mean AUC: {np.mean(aucs):.4f}, STD: {np.std(aucs):.4f}')\n",
    "print(f'Mean F1: {np.mean(f1s):.4f}, STD: {np.std(f1s):.4f}')\n",
    "print(f'Mean MCC: {np.mean(mccs):.4f}, STD: {np.std(mccs):.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4d8c3b4affbc6b8",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Taks-3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ba57adebcde740e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, matthews_corrcoef\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# **Set device configuration**\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# **Set seed for reproducibility**\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(123)\n",
    "\n",
    "lead_file_paths = {\n",
    "    \"LEAD_I\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_I.pt\",\n",
    "    \"LEAD_II\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_II.pt\",\n",
    "    \"LEAD_III\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_III.pt\",\n",
    "    \"LEAD_aVR\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_aVR.pt\",\n",
    "    \"LEAD_aVL\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_aVL.pt\",\n",
    "    \"LEAD_aVF\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_aVF.pt\",\n",
    "    \"LEAD_V1\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V1.pt\",\n",
    "    \"LEAD_V2\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V2.pt\",\n",
    "    \"LEAD_V3\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V3.pt\",\n",
    "    \"LEAD_V4\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V4.pt\",\n",
    "    \"LEAD_V5\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V5.pt\",\n",
    "    \"LEAD_V6\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V6.pt\"\n",
    "}\n",
    "labels_file_path = \"D:/ukbiobank/ECG_PAWP_UKB_Final/labels.pt\"\n",
    "\n",
    "\n",
    "# Load all lead tensors and labels\n",
    "ecg_lead_tensors = {lead: torch.load(path) for lead, path in lead_file_paths.items()}\n",
    "labels = torch.load(labels_file_path)\n",
    "\n",
    "# Ensure all leads have the same number of samples\n",
    "sample_count = len(next(iter(ecg_lead_tensors.values())))\n",
    "assert len(labels) == sample_count, \"Mismatch between number of labels and samples.\"\n",
    "for tensor in ecg_lead_tensors.values():\n",
    "    assert len(tensor) == sample_count, \"All leads must have the same number of samples.\"\n",
    "\n",
    "# **Dataset Class**\n",
    "class ECGMultiLeadDatasetWithLabels(Dataset):\n",
    "    def __init__(self, ecg_leads, labels, lead_names):\n",
    "        self.ecg_leads = {lead: ecg_leads[lead] for lead in lead_names}\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.ecg_leads.values())))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lead_data = {lead: self.ecg_leads[lead][idx].unsqueeze(0) for lead in self.ecg_leads}\n",
    "        label = self.labels[idx]\n",
    "        # Stack lead data to form a tensor of shape (num_leads, sequence_length)\n",
    "        lead_tensor = torch.cat([lead_data[lead] for lead in lead_data], dim=0)\n",
    "        return lead_tensor, label\n",
    "\n",
    "# Define the set of leads (choose between 6-lead and 12-lead configurations)\n",
    "use_6_leads = True  # Change to False for 12-lead ECG\n",
    "lead_names = (\n",
    "    [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVL\", \"LEAD_aVF\"]\n",
    "    if use_6_leads\n",
    "    else list(lead_file_paths.keys())\n",
    ")\n",
    "\n",
    "# **Initialize the dataset and dataloader**\n",
    "dataset = ECGMultiLeadDatasetWithLabels(ecg_lead_tensors, labels, lead_names)\n",
    "\n",
    "class ECGResNet1D(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes=2):\n",
    "        super(ECGResNet1D, self).__init__()\n",
    "\n",
    "        # Optimized configuration for 10-second ECG data at ~500 Hz\n",
    "        self.resnet = XResNet1d(\n",
    "            expansion=1,\n",
    "            num_layers=[3, 4, 6, 3],  # Optimized for longer ECG sequences\n",
    "            stem_k=9,                # For higher sampling rates\n",
    "            block_k=3,                # Moderate local feature extraction\n",
    "            in_ch=input_channels,\n",
    "            c_out=num_classes,\n",
    "            model_drop_r=0.1,         # Slightly higher dropout for regularization\n",
    "            verbose=False,\n",
    "            b_verbose=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "# **Training and Evaluation Functions**\n",
    "def train_classifier(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            lead_data, labels = batch\n",
    "            lead_data, labels = lead_data.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(lead_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            lead_data, labels = batch\n",
    "            lead_data, labels = lead_data.to(device), labels.to(device)\n",
    "\n",
    "            logits = model(lead_data)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "\n",
    "    return accuracy, auc_score, f1, mcc\n",
    "\n",
    "# **Cross-Validation Training**\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    train_subset = Subset(dataset, train_ids)\n",
    "    test_subset = Subset(dataset, test_ids)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Initialize XResNet1d model\n",
    "    model = ECGResNet1D(input_channels=len(lead_names), num_classes=2).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train and evaluate\n",
    "    train_classifier(model, train_loader, criterion, optimizer, epochs=50)\n",
    "    accuracy, auc_score, f1, mcc = evaluate_model(model, test_loader)\n",
    "    fold_results.append((accuracy, auc_score, f1, mcc))\n",
    "\n",
    "    print(f'Fold {fold} Results: Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}\\n')\n",
    "\n",
    "# **Calculate Metrics Across Folds**\n",
    "accuracies, aucs, f1s, mccs = zip(*fold_results)\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f}, STD: {np.std(accuracies):.4f}')\n",
    "print(f'Mean AUC: {np.mean(aucs):.4f}, STD: {np.std(aucs):.4f}')\n",
    "print(f'Mean F1: {np.mean(f1s):.4f}, STD: {np.std(f1s):.4f}')\n",
    "print(f'Mean MCC: {np.mean(mccs):.4f}, STD: {np.std(mccs):.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7698618675ef000",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
