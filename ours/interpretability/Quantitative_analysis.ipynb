{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Task-1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36c9bdc61f73f95c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define file paths for the 12-lead ECG data and labels\n",
    "lead_file_paths = {\n",
    "    \"LEAD_I\": \"../Data_processing/data_aspire_PAP_1/LEAD_I.pt\",\n",
    "    \"LEAD_II\": \"../Data_processing/data_aspire_PAP_1/LEAD_II.pt\",\n",
    "    \"LEAD_III\": \"../Data_processing/data_aspire_PAP_1/LEAD_III.pt\",\n",
    "    \"LEAD_aVR\": \"../Data_processing/data_aspire_PAP_1/LEAD_aVR.pt\",\n",
    "    \"LEAD_aVL\": \"../Data_processing/data_aspire_PAP_1/LEAD_aVL.pt\",\n",
    "    \"LEAD_aVF\": \"../Data_processing/data_aspire_PAP_1/LEAD_aVF.pt\",\n",
    "    \"LEAD_V1\": \"../Data_processing/data_aspire_PAP_1/LEAD_V1.pt\",\n",
    "    \"LEAD_V2\": \"../Data_processing/data_aspire_PAP_1/LEAD_V2.pt\",\n",
    "    \"LEAD_V3\": \"../Data_processing/data_aspire_PAP_1/LEAD_V3.pt\",\n",
    "    \"LEAD_V4\": \"../Data_processing/data_aspire_PAP_1/LEAD_V4.pt\",\n",
    "    \"LEAD_V5\": \"../Data_processing/data_aspire_PAP_1/LEAD_V5.pt\",\n",
    "    \"LEAD_V6\": \"../Data_processing/data_aspire_PAP_1/LEAD_V6.pt\"\n",
    "}\n",
    "labels_file_path = \"../Data_processing/data_aspire_PAP_1/labels.pt\"\n",
    "\n",
    "\n",
    "# Load all lead tensors and labels\n",
    "ecg_lead_tensors = {lead: torch.load(path) for lead, path in lead_file_paths.items()}\n",
    "labels = torch.load(labels_file_path)\n",
    "\n",
    "# Ensure all leads have the same number of samples as the labels\n",
    "sample_count = len(next(iter(ecg_lead_tensors.values())))\n",
    "assert len(labels) == sample_count, \"Mismatch between number of labels and samples.\"\n",
    "for tensor in ecg_lead_tensors.values():\n",
    "    assert len(tensor) == sample_count, \"All leads must have the same number of samples.\"\n",
    "\n",
    "# Define the dataset class\n",
    "class ECGMultiLeadDatasetWithLabels(Dataset):\n",
    "    def __init__(self, ecg_leads, labels):\n",
    "        self.ecg_leads = ecg_leads\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.ecg_leads.values())))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return each lead sample with the correct input shape and corresponding label\n",
    "        lead_data = {lead: self.ecg_leads[lead][idx].unsqueeze(0) for lead in self.ecg_leads}\n",
    "        label = self.labels[idx]\n",
    "        return lead_data, label\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset_2 = ECGMultiLeadDatasetWithLabels(ecg_lead_tensors, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6e8650851340381",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# **Classifier Model Class**\n",
    "class ECGLeadClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_mopoe, num_classes, use_12_leads=True):\n",
    "        super(ECGLeadClassifier, self).__init__()\n",
    "\n",
    "        # Define lead names based on the mode\n",
    "        self.use_12_leads = use_12_leads\n",
    "        self.lead_names = (\n",
    "            [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\",\n",
    "             \"LEAD_V1\", \"LEAD_V2\", \"LEAD_V3\", \"LEAD_V4\", \"LEAD_V5\", \"LEAD_V6\"]\n",
    "            if use_12_leads\n",
    "            else [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\"]\n",
    "        )\n",
    "\n",
    "        # Select encoders based on the leads to use\n",
    "        encoder_indices = range(12) if use_12_leads else range(6)\n",
    "        self.lead_encoders = nn.ModuleList([pretrained_mopoe.encoders[i] for i in encoder_indices])\n",
    "\n",
    "        self.feature_dim = pretrained_mopoe.latent_dim\n",
    "\n",
    "        # Freeze encoder weights\n",
    "        for encoder in self.lead_encoders:\n",
    "            for param in encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Define classifier network\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(len(self.lead_names) * self.feature_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, lead_data):\n",
    "        lead_features = []\n",
    "        for lead_name, encoder in zip(self.lead_names, self.lead_encoders):\n",
    "            mu, _ = encoder(lead_data[lead_name])\n",
    "            lead_features.append(mu)\n",
    "\n",
    "        combined_features = torch.cat(lead_features, dim=1)\n",
    "        logits = self.classifier(combined_features)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T18:06:06.613515400Z",
     "start_time": "2025-08-23T18:06:06.607420400Z"
    }
   },
   "id": "5c1b91763eb6a30a",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, matthews_corrcoef\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from captum.attr import IntegratedGradients\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# -------------------- Setup --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(123)\n",
    "\n",
    "# -------------------- Training Function --------------------\n",
    "def train_classifier(model, train_loader, criterion, optimizer, epochs=10, use_12_leads=True):\n",
    "    model.train()\n",
    "    lead_names = (\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\"]\n",
    "        if not use_12_leads else\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\",\n",
    "         \"LEAD_V1\", \"LEAD_V2\", \"LEAD_V3\", \"LEAD_V4\", \"LEAD_V5\", \"LEAD_V6\"]\n",
    "    )\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            lead_data, labels = batch\n",
    "            labels = labels.to(device)\n",
    "            lead_data = {lead: lead_data[lead].to(device) for lead in lead_names}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(lead_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "# -------------------- Evaluation Function --------------------\n",
    "def evaluate_model(model, data_loader, use_12_leads=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    lead_names = (\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\"]\n",
    "        if not use_12_leads else\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\",\n",
    "         \"LEAD_V1\", \"LEAD_V2\", \"LEAD_V3\", \"LEAD_V4\", \"LEAD_V5\", \"LEAD_V6\"]\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            lead_data, labels = batch\n",
    "            labels = labels.to(device)\n",
    "            lead_data = {lead: lead_data[lead].to(device) for lead in lead_names}\n",
    "            logits = model(lead_data)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    return accuracy, auc_score, f1, mcc\n",
    "\n",
    "# -------------------- IGAR Metric Function --------------------\n",
    "def compute_igar_for_fold(model, val_dataset, device, threshold=0.7):\n",
    "    model.eval()\n",
    "    igar_data = []\n",
    "    for sample_idx in range(len(val_dataset)):\n",
    "        lead_data_sample, label = val_dataset[sample_idx]\n",
    "        lead_inputs = []\n",
    "        for lead in model.lead_names:\n",
    "            tensor = lead_data_sample[lead]\n",
    "            if tensor.dim() == 2:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            tensor = tensor.to(device)\n",
    "            tensor.requires_grad_(True)\n",
    "            lead_inputs.append(tensor)\n",
    "        inputs_tuple = tuple(lead_inputs)\n",
    "        def model_wrapper(*inputs):\n",
    "            return model({lead: tensor for lead, tensor in zip(model.lead_names, inputs)})\n",
    "        with torch.no_grad():\n",
    "            logits = model_wrapper(*inputs_tuple)\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()\n",
    "        integrated_gradients = IntegratedGradients(model_wrapper)\n",
    "        attributions, _ = integrated_gradients.attribute(\n",
    "            inputs=inputs_tuple,\n",
    "            target=predicted_label,\n",
    "            return_convergence_delta=True\n",
    "        )\n",
    "        lead_igar_scores = {}\n",
    "        for idx, lead in enumerate(model.lead_names):\n",
    "            attr = attributions[idx].detach().cpu().numpy().squeeze()\n",
    "            norm_attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-10)\n",
    "            important_indices = np.where(norm_attr >= threshold)[0]\n",
    "            igar = len(important_indices) / len(attr)\n",
    "            lead_igar_scores[lead] = igar\n",
    "        total_igar = sum(lead_igar_scores.values())\n",
    "        for lead, igar in lead_igar_scores.items():\n",
    "            percent = (igar / total_igar * 100) if total_igar > 0 else 0\n",
    "            igar_data.append({\n",
    "                \"sample_idx\": sample_idx,\n",
    "                \"lead\": lead,\n",
    "                \"IGAR\": igar,\n",
    "                \"IGAR_percent\": percent\n",
    "            })\n",
    "    df = pd.DataFrame(igar_data)\n",
    "    return df\n",
    "\n",
    "# ----------- Peak-level IGAR Calculation (per sample) -----------\n",
    "def compute_peak_igar_for_sample(\n",
    "    model, lead_data_sample, device, threshold=0.6, sampling_rate=500\n",
    "):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import neurokit2 as nk\n",
    "    from captum.attr import IntegratedGradients\n",
    "    # Prepare data for model\n",
    "    for lead in lead_data_sample:\n",
    "        if lead_data_sample[lead].dim() == 2:\n",
    "            lead_data_sample[lead] = lead_data_sample[lead].unsqueeze(0)\n",
    "        lead_data_sample[lead] = lead_data_sample[lead].to(device)\n",
    "        lead_data_sample[lead].requires_grad_(True)\n",
    "    def model_wrapper(*inputs):\n",
    "        lead_data = {lead: tensor for lead, tensor in zip(model.lead_names, inputs)}\n",
    "        return model(lead_data)\n",
    "    inputs_tuple = tuple(lead_data_sample[lead] for lead in model.lead_names)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model_wrapper(*inputs_tuple)\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    integrated_gradients = IntegratedGradients(model_wrapper)\n",
    "    attributions, _ = integrated_gradients.attribute(\n",
    "        inputs=inputs_tuple,\n",
    "        target=predicted_label,\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "    peak_igar_results = {}\n",
    "    window_seconds = 0.04\n",
    "    window = int(window_seconds * sampling_rate)\n",
    "    for idx, lead in enumerate(model.lead_names):\n",
    "        ecg_signal = lead_data_sample[lead].detach().cpu().numpy().squeeze()\n",
    "        attr = attributions[idx].detach().cpu().numpy().squeeze()\n",
    "        norm_attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-10)\n",
    "        important_indices = np.where(norm_attr >= threshold)[0]\n",
    "        try:\n",
    "            ecg_cleaned = nk.ecg_clean(ecg_signal, sampling_rate=sampling_rate)\n",
    "            rpeaks_dict = nk.ecg_peaks(ecg_cleaned, sampling_rate=sampling_rate)[1]\n",
    "            r_locs = rpeaks_dict.get('ECG_R_Peaks', [])\n",
    "            if not isinstance(r_locs, (list, np.ndarray)) or len(r_locs) == 0:\n",
    "                print(f\"No R peaks detected for {lead}. Skipping.\")\n",
    "                continue\n",
    "            _, waves_peak = nk.ecg_delineate(\n",
    "                ecg_cleaned,\n",
    "                rpeaks=r_locs,\n",
    "                sampling_rate=sampling_rate,\n",
    "                method=\"dwt\"\n",
    "            )\n",
    "            def safe_peaks(peaks):\n",
    "                if peaks is None:\n",
    "                    return []\n",
    "                return [\n",
    "                    int(i) for i in peaks\n",
    "                    if (\n",
    "                        isinstance(i, (int, np.integer))\n",
    "                        or (isinstance(i, float) and not np.isnan(i))\n",
    "                    )\n",
    "                ]\n",
    "            p_locs = safe_peaks(waves_peak.get('ECG_P_Peaks', []))\n",
    "            q_locs = safe_peaks(waves_peak.get('ECG_Q_Peaks', []))\n",
    "            s_locs = safe_peaks(waves_peak.get('ECG_S_Peaks', []))\n",
    "            t_locs = safe_peaks(waves_peak.get('ECG_T_Peaks', []))\n",
    "        except Exception as e:\n",
    "            print(f\"NeuroKit2 delineation failed for {lead}: {e}\")\n",
    "            continue\n",
    "        peak_indices = {'P': p_locs, 'Q': q_locs, 'R': r_locs, 'S': s_locs, 'T': t_locs}\n",
    "        peak_import_points = {}\n",
    "        peak_igar = {}\n",
    "        for peak_type, locs in peak_indices.items():\n",
    "            window_indices = set()\n",
    "            for idx_peak in locs:\n",
    "                start = max(0, idx_peak - window)\n",
    "                end = min(len(ecg_signal), idx_peak + window + 1)\n",
    "                window_indices.update(range(start, end))\n",
    "            imp_points = [ix for ix in important_indices if ix in window_indices]\n",
    "            peak_import_points[peak_type] = len(imp_points)\n",
    "            peak_igar[peak_type] = len(imp_points) / len(ecg_signal)\n",
    "        total_imp_points = sum(peak_import_points.values())\n",
    "        peak_percent = {\n",
    "            peak_type: (peak_import_points[peak_type] / total_imp_points * 100 if total_imp_points > 0 else 0)\n",
    "            for peak_type in peak_import_points\n",
    "        }\n",
    "        result_df = pd.DataFrame({\n",
    "            'Peak': list(peak_igar.keys()),\n",
    "            'Peak_IGAR': list(peak_igar.values()),\n",
    "            'ImportantPoints': list(peak_import_points.values()),\n",
    "            'PercentOfAllImportant': list(peak_percent.values())\n",
    "        })\n",
    "        result_df = result_df.sort_values('PercentOfAllImportant', ascending=False)\n",
    "        peak_igar_results[lead] = result_df\n",
    "    return peak_igar_results\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "all_fold_igar = []\n",
    "all_peak_igar_rows = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "    print(f'FOLD {fold}\\n' + '-'*30)\n",
    "    train_subset = Subset(dataset, train_ids)\n",
    "    test_subset = Subset(dataset, test_ids)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "    # ---- Model setup ----\n",
    "    params = {'latent_dim': 256, 'input_dim_per_lead': 5000, 'num_leads': 12}\n",
    "    prior_dist = prior_expert(params['latent_dim'])\n",
    "    pretrained_mopoe = LSEMVAE(\n",
    "        prior_dist=prior_dist,\n",
    "        latent_dim=params['latent_dim'],\n",
    "        num_leads=params['num_leads'],\n",
    "        input_dim_per_lead=params['input_dim_per_lead']\n",
    "    )\n",
    "    state_dict = torch.load(\"../Main/HPC/pretrain/LS_EMVAE_with_reg_12_lead.pth\", map_location=device)\n",
    "    new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "    pretrained_mopoe.load_state_dict(new_state_dict, strict=False)\n",
    "    pretrained_mopoe.to(device)\n",
    "    model = ECGLeadClassifier(pretrained_mopoe=pretrained_mopoe, num_classes=2, use_12_leads=False).to(device)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_classifier(model, train_loader, criterion, optimizer, epochs=50, use_12_leads=False)\n",
    "    accuracy, auc_score, f1, mcc = evaluate_model(model, test_loader, use_12_leads=False)\n",
    "    fold_results.append((accuracy, auc_score, f1, mcc))\n",
    "    print(f\"Computing IGAR for fold {fold} ...\")\n",
    "    igar_df = compute_igar_for_fold(model, test_subset, device, threshold=0.7)\n",
    "    igar_df['fold'] = fold\n",
    "    all_fold_igar.append(igar_df)\n",
    "    print(f'Fold {fold} Results: Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}\\n')\n",
    "    for sample_idx in range(len(test_subset)):\n",
    "        lead_data_sample, label = test_subset[sample_idx]\n",
    "        peak_igar_results = compute_peak_igar_for_sample(\n",
    "            model, dict(lead_data_sample), device, threshold=0.6, sampling_rate=500\n",
    "        )\n",
    "        for lead, df in peak_igar_results.items():\n",
    "            df['fold'] = fold\n",
    "            df['sample_idx'] = sample_idx\n",
    "            df['lead'] = lead\n",
    "            all_peak_igar_rows.append(df)\n",
    "\n",
    "# Aggregate classification metrics\n",
    "accuracies, aucs, f1s, mccs = zip(*fold_results)\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f}, STD: {np.std(accuracies):.4f}')\n",
    "print(f'Mean AUC: {np.mean(aucs):.4f}, STD: {np.std(aucs):.4f}')\n",
    "print(f'Mean F1: {np.mean(f1s):.4f}, STD: {np.std(f1s):.4f}')\n",
    "print(f'Mean MCC: {np.mean(mccs):.4f}, STD: {np.std(mccs):.4f}')\n",
    "\n",
    "# -------- IGAR per-lead summary (mean only, no std) --------\n",
    "igar_all = pd.concat(all_fold_igar, ignore_index=True)\n",
    "igar_summary = igar_all.groupby(\"lead\").agg(\n",
    "    IGAR_mean = (\"IGAR\", \"mean\"),\n",
    "    IGAR_percent_mean = (\"IGAR_percent\", \"mean\")\n",
    ").reset_index()\n",
    "print(\"Aggregated IGAR metrics across all folds:\")\n",
    "print(igar_summary)\n",
    "\n",
    "# -------- Per-lead, per-peak IGAR summary (mean only, no std) --------\n",
    "if all_peak_igar_rows:\n",
    "    peak_igar_all = pd.concat(all_peak_igar_rows, ignore_index=True)\n",
    "    peak_igar_all = peak_igar_all.replace([np.inf, -np.inf], np.nan)\n",
    "    # Remove ImportantPoints column if not needed for summary\n",
    "    peak_igar_all = peak_igar_all.drop(columns=[\"ImportantPoints\"], errors='ignore')\n",
    "    peak_igar_summary = (\n",
    "        peak_igar_all.groupby([\"lead\", \"Peak\"])\n",
    "        .agg(\n",
    "            Peak_IGAR_mean=pd.NamedAgg(column=\"Peak_IGAR\", aggfunc=lambda x: np.nanmean(x)),\n",
    "            PercentOfAllImportant_mean=pd.NamedAgg(column=\"PercentOfAllImportant\", aggfunc=lambda x: np.nanmean(x)),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(\"Aggregated IGAR and Percent for each peak and lead (mean only):\")\n",
    "    print(peak_igar_summary)\n",
    "else:\n",
    "    print(\"No peak IGAR results to aggregate.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0136bc96dc80947",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Task-2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "747c4bfaf5b5ad6b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "lead_file_paths = {\n",
    "    \"LEAD_I\": \"../Data_processing/data_aspire_PAWP_1/LEAD_I.pt\",\n",
    "    \"LEAD_II\": \"../Data_processing/data_aspire_PAWP_1/LEAD_II.pt\",\n",
    "    \"LEAD_III\": \"../Data_processing/data_aspire_PAWP_1/LEAD_III.pt\",\n",
    "    \"LEAD_aVR\": \"../Data_processing/data_aspire_PAWP_1/LEAD_aVR.pt\",\n",
    "    \"LEAD_aVL\": \"../Data_processing/data_aspire_PAWP_1/LEAD_aVL.pt\",\n",
    "    \"LEAD_aVF\": \"../Data_processing/data_aspire_PAWP_1/LEAD_aVF.pt\",\n",
    "    \"LEAD_V1\": \"../Data_processing/data_aspire_PAWP_1/LEAD_V1.pt\",\n",
    "    \"LEAD_V2\": \"../Data_processing/data_aspire_PAWP_1/LEAD_V2.pt\",\n",
    "    \"LEAD_V3\": \"../Data_processing/data_aspire_PAWP_1/LEAD_V3.pt\",\n",
    "    \"LEAD_V4\": \"../Data_processing/data_aspire_PAWP_1/LEAD_V4.pt\",\n",
    "    \"LEAD_V5\": \"../Data_processing/data_aspire_PAWP_1/LEAD_V5.pt\",\n",
    "    \"LEAD_V6\": \"../Data_processing/data_aspire_PAWP_1/LEAD_V6.pt\"\n",
    "}\n",
    "labels_file_path = \"../Data_processing/data_aspire_PAWP_1/labels.pt\"\n",
    "\n",
    "\n",
    "# Load all lead tensors and labels\n",
    "ecg_lead_tensors = {lead: torch.load(path) for lead, path in lead_file_paths.items()}\n",
    "labels = torch.load(labels_file_path)\n",
    "\n",
    "# Ensure all leads have the same number of samples as the labels\n",
    "sample_count = len(next(iter(ecg_lead_tensors.values())))\n",
    "assert len(labels) == sample_count, \"Mismatch between number of labels and samples.\"\n",
    "for tensor in ecg_lead_tensors.values():\n",
    "    assert len(tensor) == sample_count, \"All leads must have the same number of samples.\"\n",
    "\n",
    "# Define the dataset class\n",
    "class ECGMultiLeadDatasetWithLabels(Dataset):\n",
    "    def __init__(self, ecg_leads, labels):\n",
    "        self.ecg_leads = ecg_leads\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.ecg_leads.values())))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return each lead sample with the correct input shape and corresponding label\n",
    "        lead_data = {lead: self.ecg_leads[lead][idx].unsqueeze(0) for lead in self.ecg_leads}\n",
    "        label = self.labels[idx]\n",
    "        return lead_data, label\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset_2 = ECGMultiLeadDatasetWithLabels(ecg_lead_tensors, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3389c369aee3934",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, matthews_corrcoef\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from captum.attr import IntegratedGradients\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# -------------------- Setup --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(123)\n",
    "\n",
    "# -------------------- Training Function --------------------\n",
    "def train_classifier(model, train_loader, criterion, optimizer, epochs=10, use_12_leads=True):\n",
    "    model.train()\n",
    "    lead_names = (\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\"]\n",
    "        if not use_12_leads else\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\",\n",
    "         \"LEAD_V1\", \"LEAD_V2\", \"LEAD_V3\", \"LEAD_V4\", \"LEAD_V5\", \"LEAD_V6\"]\n",
    "    )\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            lead_data, labels = batch\n",
    "            labels = labels.to(device)\n",
    "            lead_data = {lead: lead_data[lead].to(device) for lead in lead_names}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(lead_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "# -------------------- Evaluation Function --------------------\n",
    "def evaluate_model(model, data_loader, use_12_leads=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    lead_names = (\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\"]\n",
    "        if not use_12_leads else\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\",\n",
    "         \"LEAD_V1\", \"LEAD_V2\", \"LEAD_V3\", \"LEAD_V4\", \"LEAD_V5\", \"LEAD_V6\"]\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            lead_data, labels = batch\n",
    "            labels = labels.to(device)\n",
    "            lead_data = {lead: lead_data[lead].to(device) for lead in lead_names}\n",
    "            logits = model(lead_data)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    return accuracy, auc_score, f1, mcc\n",
    "\n",
    "# -------------------- IGAR Metric Function --------------------\n",
    "def compute_igar_for_fold(model, val_dataset, device, threshold=0.7):\n",
    "    model.eval()\n",
    "    igar_data = []\n",
    "    for sample_idx in range(len(val_dataset)):\n",
    "        lead_data_sample, label = val_dataset[sample_idx]\n",
    "        lead_inputs = []\n",
    "        for lead in model.lead_names:\n",
    "            tensor = lead_data_sample[lead]\n",
    "            if tensor.dim() == 2:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            tensor = tensor.to(device)\n",
    "            tensor.requires_grad_(True)\n",
    "            lead_inputs.append(tensor)\n",
    "        inputs_tuple = tuple(lead_inputs)\n",
    "        def model_wrapper(*inputs):\n",
    "            return model({lead: tensor for lead, tensor in zip(model.lead_names, inputs)})\n",
    "        with torch.no_grad():\n",
    "            logits = model_wrapper(*inputs_tuple)\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()\n",
    "        integrated_gradients = IntegratedGradients(model_wrapper)\n",
    "        attributions, _ = integrated_gradients.attribute(\n",
    "            inputs=inputs_tuple,\n",
    "            target=predicted_label,\n",
    "            return_convergence_delta=True\n",
    "        )\n",
    "        lead_igar_scores = {}\n",
    "        for idx, lead in enumerate(model.lead_names):\n",
    "            attr = attributions[idx].detach().cpu().numpy().squeeze()\n",
    "            norm_attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-10)\n",
    "            important_indices = np.where(norm_attr >= threshold)[0]\n",
    "            igar = len(important_indices) / len(attr)\n",
    "            lead_igar_scores[lead] = igar\n",
    "        total_igar = sum(lead_igar_scores.values())\n",
    "        for lead, igar in lead_igar_scores.items():\n",
    "            percent = (igar / total_igar * 100) if total_igar > 0 else 0\n",
    "            igar_data.append({\n",
    "                \"sample_idx\": sample_idx,\n",
    "                \"lead\": lead,\n",
    "                \"IGAR\": igar,\n",
    "                \"IGAR_percent\": percent\n",
    "            })\n",
    "    df = pd.DataFrame(igar_data)\n",
    "    return df\n",
    "\n",
    "# ----------- Peak-level IGAR Calculation (per sample) -----------\n",
    "def compute_peak_igar_for_sample(\n",
    "    model, lead_data_sample, device, threshold=0.6, sampling_rate=500\n",
    "):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import neurokit2 as nk\n",
    "    from captum.attr import IntegratedGradients\n",
    "    # Prepare data for model\n",
    "    for lead in lead_data_sample:\n",
    "        if lead_data_sample[lead].dim() == 2:\n",
    "            lead_data_sample[lead] = lead_data_sample[lead].unsqueeze(0)\n",
    "        lead_data_sample[lead] = lead_data_sample[lead].to(device)\n",
    "        lead_data_sample[lead].requires_grad_(True)\n",
    "    def model_wrapper(*inputs):\n",
    "        lead_data = {lead: tensor for lead, tensor in zip(model.lead_names, inputs)}\n",
    "        return model(lead_data)\n",
    "    inputs_tuple = tuple(lead_data_sample[lead] for lead in model.lead_names)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model_wrapper(*inputs_tuple)\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    integrated_gradients = IntegratedGradients(model_wrapper)\n",
    "    attributions, _ = integrated_gradients.attribute(\n",
    "        inputs=inputs_tuple,\n",
    "        target=predicted_label,\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "    peak_igar_results = {}\n",
    "    window_seconds = 0.04\n",
    "    window = int(window_seconds * sampling_rate)\n",
    "    for idx, lead in enumerate(model.lead_names):\n",
    "        ecg_signal = lead_data_sample[lead].detach().cpu().numpy().squeeze()\n",
    "        attr = attributions[idx].detach().cpu().numpy().squeeze()\n",
    "        norm_attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-10)\n",
    "        important_indices = np.where(norm_attr >= threshold)[0]\n",
    "        try:\n",
    "            ecg_cleaned = nk.ecg_clean(ecg_signal, sampling_rate=sampling_rate)\n",
    "            rpeaks_dict = nk.ecg_peaks(ecg_cleaned, sampling_rate=sampling_rate)[1]\n",
    "            r_locs = rpeaks_dict.get('ECG_R_Peaks', [])\n",
    "            if not isinstance(r_locs, (list, np.ndarray)) or len(r_locs) == 0:\n",
    "                print(f\"No R peaks detected for {lead}. Skipping.\")\n",
    "                continue\n",
    "            _, waves_peak = nk.ecg_delineate(\n",
    "                ecg_cleaned,\n",
    "                rpeaks=r_locs,\n",
    "                sampling_rate=sampling_rate,\n",
    "                method=\"dwt\"\n",
    "            )\n",
    "            def safe_peaks(peaks):\n",
    "                if peaks is None:\n",
    "                    return []\n",
    "                return [\n",
    "                    int(i) for i in peaks\n",
    "                    if (\n",
    "                        isinstance(i, (int, np.integer))\n",
    "                        or (isinstance(i, float) and not np.isnan(i))\n",
    "                    )\n",
    "                ]\n",
    "            p_locs = safe_peaks(waves_peak.get('ECG_P_Peaks', []))\n",
    "            q_locs = safe_peaks(waves_peak.get('ECG_Q_Peaks', []))\n",
    "            s_locs = safe_peaks(waves_peak.get('ECG_S_Peaks', []))\n",
    "            t_locs = safe_peaks(waves_peak.get('ECG_T_Peaks', []))\n",
    "        except Exception as e:\n",
    "            print(f\"NeuroKit2 delineation failed for {lead}: {e}\")\n",
    "            continue\n",
    "        peak_indices = {'P': p_locs, 'Q': q_locs, 'R': r_locs, 'S': s_locs, 'T': t_locs}\n",
    "        peak_import_points = {}\n",
    "        peak_igar = {}\n",
    "        for peak_type, locs in peak_indices.items():\n",
    "            window_indices = set()\n",
    "            for idx_peak in locs:\n",
    "                start = max(0, idx_peak - window)\n",
    "                end = min(len(ecg_signal), idx_peak + window + 1)\n",
    "                window_indices.update(range(start, end))\n",
    "            imp_points = [ix for ix in important_indices if ix in window_indices]\n",
    "            peak_import_points[peak_type] = len(imp_points)\n",
    "            peak_igar[peak_type] = len(imp_points) / len(ecg_signal)\n",
    "        total_imp_points = sum(peak_import_points.values())\n",
    "        peak_percent = {\n",
    "            peak_type: (peak_import_points[peak_type] / total_imp_points * 100 if total_imp_points > 0 else 0)\n",
    "            for peak_type in peak_import_points\n",
    "        }\n",
    "        result_df = pd.DataFrame({\n",
    "            'Peak': list(peak_igar.keys()),\n",
    "            'Peak_IGAR': list(peak_igar.values()),\n",
    "            'ImportantPoints': list(peak_import_points.values()),\n",
    "            'PercentOfAllImportant': list(peak_percent.values())\n",
    "        })\n",
    "        result_df = result_df.sort_values('PercentOfAllImportant', ascending=False)\n",
    "        peak_igar_results[lead] = result_df\n",
    "    return peak_igar_results\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "all_fold_igar = []\n",
    "all_peak_igar_rows = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "    print(f'FOLD {fold}\\n' + '-'*30)\n",
    "    train_subset = Subset(dataset, train_ids)\n",
    "    test_subset = Subset(dataset, test_ids)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "    # ---- Model setup ----\n",
    "    params = {'latent_dim': 256, 'input_dim_per_lead': 5000, 'num_leads': 12}\n",
    "    prior_dist = prior_expert(params['latent_dim'])\n",
    "    pretrained_mopoe = LSEMVAE(\n",
    "        prior_dist=prior_dist,\n",
    "        latent_dim=params['latent_dim'],\n",
    "        num_leads=params['num_leads'],\n",
    "        input_dim_per_lead=params['input_dim_per_lead']\n",
    "    )\n",
    "    state_dict = torch.load(\"../Main/HPC/pretrain/LS_EMVAE_with_reg_12_lead.pth\", map_location=device)\n",
    "    new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "    pretrained_mopoe.load_state_dict(new_state_dict, strict=False)\n",
    "    pretrained_mopoe.to(device)\n",
    "    model = ECGLeadClassifier(pretrained_mopoe=pretrained_mopoe, num_classes=2, use_12_leads=False).to(device)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_classifier(model, train_loader, criterion, optimizer, epochs=50, use_12_leads=False)\n",
    "    accuracy, auc_score, f1, mcc = evaluate_model(model, test_loader, use_12_leads=False)\n",
    "    fold_results.append((accuracy, auc_score, f1, mcc))\n",
    "    print(f\"Computing IGAR for fold {fold} ...\")\n",
    "    igar_df = compute_igar_for_fold(model, test_subset, device, threshold=0.7)\n",
    "    igar_df['fold'] = fold\n",
    "    all_fold_igar.append(igar_df)\n",
    "    print(f'Fold {fold} Results: Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}\\n')\n",
    "\n",
    "    for sample_idx in range(len(test_subset)):\n",
    "        lead_data_sample, label = test_subset[sample_idx]\n",
    "        peak_igar_results = compute_peak_igar_for_sample(\n",
    "            model, dict(lead_data_sample), device, threshold=0.6, sampling_rate=500\n",
    "        )\n",
    "        for lead, df in peak_igar_results.items():\n",
    "            df['fold'] = fold\n",
    "            df['sample_idx'] = sample_idx\n",
    "            df['lead'] = lead\n",
    "            all_peak_igar_rows.append(df)\n",
    "\n",
    "# Aggregate classification metrics\n",
    "accuracies, aucs, f1s, mccs = zip(*fold_results)\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f}, STD: {np.std(accuracies):.4f}')\n",
    "print(f'Mean AUC: {np.mean(aucs):.4f}, STD: {np.std(aucs):.4f}')\n",
    "print(f'Mean F1: {np.mean(f1s):.4f}, STD: {np.std(f1s):.4f}')\n",
    "print(f'Mean MCC: {np.mean(mccs):.4f}, STD: {np.std(mccs):.4f}')\n",
    "\n",
    "# -------- IGAR per-lead summary (mean only, no std) --------\n",
    "igar_all = pd.concat(all_fold_igar, ignore_index=True)\n",
    "igar_summary = igar_all.groupby(\"lead\").agg(\n",
    "    IGAR_mean = (\"IGAR\", \"mean\"),\n",
    "    IGAR_percent_mean = (\"IGAR_percent\", \"mean\")\n",
    ").reset_index()\n",
    "print(\"Aggregated IGAR metrics across all folds:\")\n",
    "print(igar_summary)\n",
    "\n",
    "# -------- Per-lead, per-peak IGAR summary (mean only, no std) --------\n",
    "if all_peak_igar_rows:\n",
    "    peak_igar_all = pd.concat(all_peak_igar_rows, ignore_index=True)\n",
    "    peak_igar_all = peak_igar_all.replace([np.inf, -np.inf], np.nan)\n",
    "    # Remove ImportantPoints column if not needed for summary\n",
    "    peak_igar_all = peak_igar_all.drop(columns=[\"ImportantPoints\"], errors='ignore')\n",
    "    peak_igar_summary = (\n",
    "        peak_igar_all.groupby([\"lead\", \"Peak\"])\n",
    "        .agg(\n",
    "            Peak_IGAR_mean=pd.NamedAgg(column=\"Peak_IGAR\", aggfunc=lambda x: np.nanmean(x)),\n",
    "            PercentOfAllImportant_mean=pd.NamedAgg(column=\"PercentOfAllImportant\", aggfunc=lambda x: np.nanmean(x)),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(\"Aggregated IGAR and Percent for each peak and lead (mean only):\")\n",
    "    print(peak_igar_summary)\n",
    "else:\n",
    "    print(\"No peak IGAR results to aggregate.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5ab50604f931b2f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Task-3"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34505bdb32110662"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# File paths\n",
    "lead_file_paths = {\n",
    "    \"LEAD_I\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_I.pt\",\n",
    "    \"LEAD_II\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_II.pt\",\n",
    "    \"LEAD_III\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_III.pt\",\n",
    "    \"LEAD_aVR\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_aVR.pt\",\n",
    "    \"LEAD_aVL\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_aVL.pt\",\n",
    "    \"LEAD_aVF\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_aVF.pt\",\n",
    "    \"LEAD_V1\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V1.pt\",\n",
    "    \"LEAD_V2\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V2.pt\",\n",
    "    \"LEAD_V3\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V3.pt\",\n",
    "    \"LEAD_V4\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V4.pt\",\n",
    "    \"LEAD_V5\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V5.pt\",\n",
    "    \"LEAD_V6\": \"D:/ukbiobank/ECG_PAWP_UKB_Final/LEAD_V6.pt\"\n",
    "}\n",
    "labels_file_path = \"D:/ukbiobank/ECG_PAWP_UKB_Final/labels.pt\"\n",
    "\n",
    "# Load all lead tensors and labels\n",
    "ecg_lead_tensors = {lead: torch.load(path) for lead, path in lead_file_paths.items()}\n",
    "labels = torch.load(labels_file_path)\n",
    "\n",
    "# Ensure all leads have the same number of samples as the labels\n",
    "sample_count = len(next(iter(ecg_lead_tensors.values())))\n",
    "print(len(labels))\n",
    "print(sample_count)\n",
    "assert len(labels) == sample_count, \"Mismatch between number of labels and samples.\"\n",
    "for tensor in ecg_lead_tensors.values():\n",
    "    assert len(tensor) == sample_count, \"All leads must have the same number of samples.\"\n",
    "\n",
    "# Define the dataset class\n",
    "class ECGMultiLeadDatasetWithLabels(Dataset):\n",
    "    def __init__(self, ecg_leads, labels):\n",
    "        self.ecg_leads = ecg_leads\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(next(iter(self.ecg_leads.values())))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return each lead sample with the correct input shape and corresponding label\n",
    "        lead_data = {lead: self.ecg_leads[lead][idx].unsqueeze(0) for lead in self.ecg_leads}\n",
    "        label = self.labels[idx]\n",
    "        return lead_data, label\n",
    "\n",
    "# Initialize the dataset and dataloader\n",
    "dataset = ECGMultiLeadDatasetWithLabels(ecg_lead_tensors, labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "873d85f0e245a72c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, matthews_corrcoef\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from captum.attr import IntegratedGradients\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# -------------------- Setup --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_seed(123)\n",
    "\n",
    "# -------------------- Training Function --------------------\n",
    "def train_classifier(model, train_loader, criterion, optimizer, epochs=10, use_12_leads=True):\n",
    "    model.train()\n",
    "    lead_names = (\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\"]\n",
    "        if not use_12_leads else\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\",\n",
    "         \"LEAD_V1\", \"LEAD_V2\", \"LEAD_V3\", \"LEAD_V4\", \"LEAD_V5\", \"LEAD_V6\"]\n",
    "    )\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            lead_data, labels = batch\n",
    "            labels = labels.to(device)\n",
    "            lead_data = {lead: lead_data[lead].to(device) for lead in lead_names}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(lead_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "\n",
    "# -------------------- Evaluation Function --------------------\n",
    "def evaluate_model(model, data_loader, use_12_leads=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    lead_names = (\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\"]\n",
    "        if not use_12_leads else\n",
    "        [\"LEAD_I\", \"LEAD_II\", \"LEAD_III\", \"LEAD_aVR\", \"LEAD_aVF\", \"LEAD_aVL\",\n",
    "         \"LEAD_V1\", \"LEAD_V2\", \"LEAD_V3\", \"LEAD_V4\", \"LEAD_V5\", \"LEAD_V6\"]\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            lead_data, labels = batch\n",
    "            labels = labels.to(device)\n",
    "            lead_data = {lead: lead_data[lead].to(device) for lead in lead_names}\n",
    "            logits = model(lead_data)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    return accuracy, auc_score, f1, mcc\n",
    "\n",
    "# -------------------- IGAR Metric Function --------------------\n",
    "def compute_igar_for_fold(model, val_dataset, device, threshold=0.7):\n",
    "    model.eval()\n",
    "    igar_data = []\n",
    "    for sample_idx in range(len(val_dataset)):\n",
    "        lead_data_sample, label = val_dataset[sample_idx]\n",
    "        lead_inputs = []\n",
    "        for lead in model.lead_names:\n",
    "            tensor = lead_data_sample[lead]\n",
    "            if tensor.dim() == 2:\n",
    "                tensor = tensor.unsqueeze(0)\n",
    "            tensor = tensor.to(device)\n",
    "            tensor.requires_grad_(True)\n",
    "            lead_inputs.append(tensor)\n",
    "        inputs_tuple = tuple(lead_inputs)\n",
    "        def model_wrapper(*inputs):\n",
    "            return model({lead: tensor for lead, tensor in zip(model.lead_names, inputs)})\n",
    "        with torch.no_grad():\n",
    "            logits = model_wrapper(*inputs_tuple)\n",
    "        predicted_label = torch.argmax(logits, dim=1).item()\n",
    "        integrated_gradients = IntegratedGradients(model_wrapper)\n",
    "        attributions, _ = integrated_gradients.attribute(\n",
    "            inputs=inputs_tuple,\n",
    "            target=predicted_label,\n",
    "            return_convergence_delta=True\n",
    "        )\n",
    "        lead_igar_scores = {}\n",
    "        for idx, lead in enumerate(model.lead_names):\n",
    "            attr = attributions[idx].detach().cpu().numpy().squeeze()\n",
    "            norm_attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-10)\n",
    "            important_indices = np.where(norm_attr >= threshold)[0]\n",
    "            igar = len(important_indices) / len(attr)\n",
    "            lead_igar_scores[lead] = igar\n",
    "        total_igar = sum(lead_igar_scores.values())\n",
    "        for lead, igar in lead_igar_scores.items():\n",
    "            percent = (igar / total_igar * 100) if total_igar > 0 else 0\n",
    "            igar_data.append({\n",
    "                \"sample_idx\": sample_idx,\n",
    "                \"lead\": lead,\n",
    "                \"IGAR\": igar,\n",
    "                \"IGAR_percent\": percent\n",
    "            })\n",
    "    df = pd.DataFrame(igar_data)\n",
    "    return df\n",
    "\n",
    "# ----------- Peak-level IGAR Calculation (per sample) -----------\n",
    "def compute_peak_igar_for_sample(\n",
    "    model, lead_data_sample, device, threshold=0.6, sampling_rate=500\n",
    "):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import neurokit2 as nk\n",
    "    from captum.attr import IntegratedGradients\n",
    "    # Prepare data for model\n",
    "    for lead in lead_data_sample:\n",
    "        if lead_data_sample[lead].dim() == 2:\n",
    "            lead_data_sample[lead] = lead_data_sample[lead].unsqueeze(0)\n",
    "        lead_data_sample[lead] = lead_data_sample[lead].to(device)\n",
    "        lead_data_sample[lead].requires_grad_(True)\n",
    "    def model_wrapper(*inputs):\n",
    "        lead_data = {lead: tensor for lead, tensor in zip(model.lead_names, inputs)}\n",
    "        return model(lead_data)\n",
    "    inputs_tuple = tuple(lead_data_sample[lead] for lead in model.lead_names)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model_wrapper(*inputs_tuple)\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    integrated_gradients = IntegratedGradients(model_wrapper)\n",
    "    attributions, _ = integrated_gradients.attribute(\n",
    "        inputs=inputs_tuple,\n",
    "        target=predicted_label,\n",
    "        return_convergence_delta=True\n",
    "    )\n",
    "    peak_igar_results = {}\n",
    "    window_seconds = 0.04\n",
    "    window = int(window_seconds * sampling_rate)\n",
    "    for idx, lead in enumerate(model.lead_names):\n",
    "        ecg_signal = lead_data_sample[lead].detach().cpu().numpy().squeeze()\n",
    "        attr = attributions[idx].detach().cpu().numpy().squeeze()\n",
    "        norm_attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-10)\n",
    "        important_indices = np.where(norm_attr >= threshold)[0]\n",
    "        try:\n",
    "            ecg_cleaned = nk.ecg_clean(ecg_signal, sampling_rate=sampling_rate)\n",
    "            rpeaks_dict = nk.ecg_peaks(ecg_cleaned, sampling_rate=sampling_rate)[1]\n",
    "            r_locs = rpeaks_dict.get('ECG_R_Peaks', [])\n",
    "            if not isinstance(r_locs, (list, np.ndarray)) or len(r_locs) == 0:\n",
    "                print(f\"No R peaks detected for {lead}. Skipping.\")\n",
    "                continue\n",
    "            _, waves_peak = nk.ecg_delineate(\n",
    "                ecg_cleaned,\n",
    "                rpeaks=r_locs,\n",
    "                sampling_rate=sampling_rate,\n",
    "                method=\"dwt\"\n",
    "            )\n",
    "            def safe_peaks(peaks):\n",
    "                if peaks is None:\n",
    "                    return []\n",
    "                return [\n",
    "                    int(i) for i in peaks\n",
    "                    if (\n",
    "                        isinstance(i, (int, np.integer))\n",
    "                        or (isinstance(i, float) and not np.isnan(i))\n",
    "                    )\n",
    "                ]\n",
    "            p_locs = safe_peaks(waves_peak.get('ECG_P_Peaks', []))\n",
    "            q_locs = safe_peaks(waves_peak.get('ECG_Q_Peaks', []))\n",
    "            s_locs = safe_peaks(waves_peak.get('ECG_S_Peaks', []))\n",
    "            t_locs = safe_peaks(waves_peak.get('ECG_T_Peaks', []))\n",
    "        except Exception as e:\n",
    "            print(f\"NeuroKit2 delineation failed for {lead}: {e}\")\n",
    "            continue\n",
    "        peak_indices = {'P': p_locs, 'Q': q_locs, 'R': r_locs, 'S': s_locs, 'T': t_locs}\n",
    "        peak_import_points = {}\n",
    "        peak_igar = {}\n",
    "        for peak_type, locs in peak_indices.items():\n",
    "            window_indices = set()\n",
    "            for idx_peak in locs:\n",
    "                start = max(0, idx_peak - window)\n",
    "                end = min(len(ecg_signal), idx_peak + window + 1)\n",
    "                window_indices.update(range(start, end))\n",
    "            imp_points = [ix for ix in important_indices if ix in window_indices]\n",
    "            peak_import_points[peak_type] = len(imp_points)\n",
    "            peak_igar[peak_type] = len(imp_points) / len(ecg_signal)\n",
    "        total_imp_points = sum(peak_import_points.values())\n",
    "        peak_percent = {\n",
    "            peak_type: (peak_import_points[peak_type] / total_imp_points * 100 if total_imp_points > 0 else 0)\n",
    "            for peak_type in peak_import_points\n",
    "        }\n",
    "        result_df = pd.DataFrame({\n",
    "            'Peak': list(peak_igar.keys()),\n",
    "            'Peak_IGAR': list(peak_igar.values()),\n",
    "            'ImportantPoints': list(peak_import_points.values()),\n",
    "            'PercentOfAllImportant': list(peak_percent.values())\n",
    "        })\n",
    "        result_df = result_df.sort_values('PercentOfAllImportant', ascending=False)\n",
    "        peak_igar_results[lead] = result_df\n",
    "    return peak_igar_results\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "all_fold_igar = []\n",
    "all_peak_igar_rows = []\n",
    "\n",
    "for fold, (train_ids, test_ids) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
    "    print(f'FOLD {fold}\\n' + '-'*30)\n",
    "    train_subset = Subset(dataset, train_ids)\n",
    "    test_subset = Subset(dataset, test_ids)\n",
    "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n",
    "    # ---- Model setup ----\n",
    "    params = {'latent_dim': 256, 'input_dim_per_lead': 5000, 'num_leads': 12}\n",
    "    prior_dist = prior_expert(params['latent_dim'])\n",
    "    pretrained_mopoe = MoPoE(\n",
    "        prior_dist=prior_dist,\n",
    "        latent_dim=params['latent_dim'],\n",
    "        num_leads=params['num_leads'],\n",
    "        input_dim_per_lead=params['input_dim_per_lead']\n",
    "    )\n",
    "    state_dict = torch.load(\"../Main/HPC/pretrain/LS_EMVAE_with_reg_12_lead.pth\", map_location=device)\n",
    "    new_state_dict = {k.replace(\"_orig_mod.\", \"\"): v for k, v in state_dict.items()}\n",
    "    pretrained_mopoe.load_state_dict(new_state_dict, strict=False)\n",
    "    pretrained_mopoe.to(device)\n",
    "    model = ECGLeadClassifier(pretrained_mopoe=pretrained_mopoe, num_classes=2, use_12_leads=False).to(device)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_classifier(model, train_loader, criterion, optimizer, epochs=50, use_12_leads=False)\n",
    "    accuracy, auc_score, f1, mcc = evaluate_model(model, test_loader, use_12_leads=False)\n",
    "    fold_results.append((accuracy, auc_score, f1, mcc))\n",
    "    print(f\"Computing IGAR for fold {fold} ...\")\n",
    "    igar_df = compute_igar_for_fold(model, test_subset, device, threshold=0.7)\n",
    "    igar_df['fold'] = fold\n",
    "    all_fold_igar.append(igar_df)\n",
    "    print(f'Fold {fold} Results: Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}, F1: {f1:.4f}, MCC: {mcc:.4f}\\n')\n",
    "    for sample_idx in range(len(test_subset)):\n",
    "        lead_data_sample, label = test_subset[sample_idx]\n",
    "        peak_igar_results = compute_peak_igar_for_sample(\n",
    "            model, dict(lead_data_sample), device, threshold=0.6, sampling_rate=500\n",
    "        )\n",
    "        for lead, df in peak_igar_results.items():\n",
    "            df['fold'] = fold\n",
    "            df['sample_idx'] = sample_idx\n",
    "            df['lead'] = lead\n",
    "            all_peak_igar_rows.append(df)\n",
    "\n",
    "# Aggregate classification metrics\n",
    "accuracies, aucs, f1s, mccs = zip(*fold_results)\n",
    "print(f'Mean Accuracy: {np.mean(accuracies):.4f}, STD: {np.std(accuracies):.4f}')\n",
    "print(f'Mean AUC: {np.mean(aucs):.4f}, STD: {np.std(aucs):.4f}')\n",
    "print(f'Mean F1: {np.mean(f1s):.4f}, STD: {np.std(f1s):.4f}')\n",
    "print(f'Mean MCC: {np.mean(mccs):.4f}, STD: {np.std(mccs):.4f}')\n",
    "\n",
    "# -------- IGAR per-lead summary (mean only, no std) --------\n",
    "igar_all = pd.concat(all_fold_igar, ignore_index=True)\n",
    "igar_summary = igar_all.groupby(\"lead\").agg(\n",
    "    IGAR_mean = (\"IGAR\", \"mean\"),\n",
    "    IGAR_percent_mean = (\"IGAR_percent\", \"mean\")\n",
    ").reset_index()\n",
    "print(\"Aggregated IGAR metrics across all folds:\")\n",
    "print(igar_summary)\n",
    "\n",
    "# -------- Per-lead, per-peak IGAR summary (mean only, no std) --------\n",
    "if all_peak_igar_rows:\n",
    "    peak_igar_all = pd.concat(all_peak_igar_rows, ignore_index=True)\n",
    "    peak_igar_all = peak_igar_all.replace([np.inf, -np.inf], np.nan)\n",
    "    # Remove ImportantPoints column if not needed for summary\n",
    "    peak_igar_all = peak_igar_all.drop(columns=[\"ImportantPoints\"], errors='ignore')\n",
    "    peak_igar_summary = (\n",
    "        peak_igar_all.groupby([\"lead\", \"Peak\"])\n",
    "        .agg(\n",
    "            Peak_IGAR_mean=pd.NamedAgg(column=\"Peak_IGAR\", aggfunc=lambda x: np.nanmean(x)),\n",
    "            PercentOfAllImportant_mean=pd.NamedAgg(column=\"PercentOfAllImportant\", aggfunc=lambda x: np.nanmean(x)),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    print(\"Aggregated IGAR and Percent for each peak and lead (mean only):\")\n",
    "    print(peak_igar_summary)\n",
    "else:\n",
    "    print(\"No peak IGAR results to aggregate.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a7541c701b400d4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c2d8ecba9d5a783"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# First chart data (original values)\n",
    "lead_data_1 = {\n",
    "    \"LEAD-I\": 12.564727,\n",
    "    \"LEAD-II\": 21.844531,\n",
    "    \"LEAD-III\": 13.018823,\n",
    "    \"LEAD-aVF\": 11.387362,\n",
    "    \"LEAD-aVL\": 13.751365,\n",
    "    \"LEAD-aVR\": 27.433193\n",
    "}\n",
    "\n",
    "peak_data_1 = {\n",
    "    \"LEAD-I\":     [3.036466, 9.138413, 49.824495, 30.227844, 7.548567],\n",
    "    \"LEAD-II\":    [4.417913, 8.735130, 45.335424, 29.172962, 11.544920],\n",
    "    \"LEAD-III\":   [5.750188, 16.674821, 50.809959, 16.893581, 8.047049],\n",
    "    \"LEAD-aVF\":   [2.444300, 11.491595, 47.125567, 33.095187, 5.729065],\n",
    "    \"LEAD-aVL\":   [6.625456, 12.417031, 48.337901, 21.719252, 10.328932],\n",
    "    \"LEAD-aVR\":   [3.843334, 32.156304, 41.643393, 17.645506, 4.141337]\n",
    "}\n",
    "\n",
    "# Second chart data (IGAR_percent_mean and PercentOfAllImportant_mean)\n",
    "lead_data_2 = {\n",
    "    \"LEAD-I\": 12.130735,\n",
    "    \"LEAD-II\": 22.511896,\n",
    "    \"LEAD-III\": 15.411085,\n",
    "    \"LEAD-aVF\": 14.734527,\n",
    "    \"LEAD-aVL\": 19.132485,\n",
    "    \"LEAD-aVR\": 16.079271\n",
    "}\n",
    "\n",
    "peak_data_2 = {\n",
    "    \"LEAD-I\":     [3.790666, 9.092196, 46.494400, 32.260213, 8.073088],\n",
    "    \"LEAD-II\":    [5.751059, 8.778773, 43.917717, 29.813044, 11.300168],\n",
    "    \"LEAD-III\":   [5.370212, 16.532282, 54.104467, 15.730855, 7.378531],\n",
    "    \"LEAD-aVF\":   [2.228947, 10.425398, 45.050057, 36.523867, 5.771732],\n",
    "    \"LEAD-aVL\":   [7.383158, 12.135778, 49.124514, 20.593080, 10.026007],\n",
    "    \"LEAD-aVR\":   [3.511032, 32.936929, 42.726368, 16.250718, 3.991012]\n",
    "}\n",
    "\n",
    "# Base colors\n",
    "base_colors = {\n",
    "    \"LEAD-I\": \"#CB4335\",\n",
    "    \"LEAD-II\": \"#2E86C1\",\n",
    "    \"LEAD-III\": \"#239B56\",\n",
    "    \"LEAD-aVF\": \"#AF7AC5\",\n",
    "    \"LEAD-aVL\": \"#F39C12\",\n",
    "    \"LEAD-aVR\": \"#7F8C8D\"\n",
    "}\n",
    "\n",
    "lighter_colors = {\n",
    "    \"LEAD-I\": [\"#EC7063\", \"#F1948A\", \"#F5B7B1\", \"#FADBD8\", \"#FDEDEC\"],\n",
    "    \"LEAD-II\": [\"#5DADE2\", \"#85C1E9\", \"#AED6F1\", \"#D6EAF8\", \"#EBF5FB\"],\n",
    "    \"LEAD-III\": [\"#58D68D\", \"#82E0AA\", \"#ABEBC6\", \"#D5F5E3\", \"#EAFAF1\"],\n",
    "    \"LEAD-aVF\": [\"#C39BD3\", \"#D7BDE2\", \"#E8DAEF\", \"#F4ECF7\", \"#FBF6FB\"],\n",
    "    \"LEAD-aVL\": [\"#F8C471\", \"#FAD7A0\", \"#FDEBD0\", \"#FEF5E7\", \"#FFFCF5\"],\n",
    "    \"LEAD-aVR\": [\"#95A5A6\", \"#B2BABB\", \"#CCD1D1\", \"#E5E8E8\", \"#F7F9F9\"]\n",
    "}\n",
    "\n",
    "base_colors = {\n",
    "    \"LEAD-I\":   \"#3b7fba\",   # Softened Blue (was #2066a8)\n",
    "    \"LEAD-II\":  \"#7cab5f\",   # Softer Olive Green (was #6a994e)\n",
    "    \"LEAD-III\": \"#f3cd94\",   # Slightly lighter Gold (was #f0c987)\n",
    "    \"LEAD-aVF\": \"#6fb7aa\",   # Softer Teal (was #59a89c)\n",
    "    \"LEAD-aVL\": \"#b86ab7\",   # Softer Purple (was #a559aa)\n",
    "    \"LEAD-aVR\": \"#e6434c\"    # Softer Red (was #e02b35)\n",
    "}\n",
    "\n",
    "\n",
    "lighter_colors = {\n",
    "    \"LEAD-I\":    [\"#5a97cb\", \"#7aafdc\", \"#9ac7ed\", \"#badaf2\", \"#def0fa\"],\n",
    "    \"LEAD-II\":   [\"#90bb7c\", \"#a5cb99\", \"#badcb6\", \"#cfecd3\", \"#e4fce9\"],\n",
    "    \"LEAD-III\":  [\"#f5d6a9\", \"#f7e0be\", \"#fae9d4\", \"#fcf3e9\", \"#fefcf5\"],\n",
    "    \"LEAD-aVF\":  [\"#8bc6bc\", \"#a7d6ce\", \"#c3e5e1\", \"#dff5f3\", \"#effcfa\"],\n",
    "    \"LEAD-aVL\":  [\"#c885c7\", \"#d7a0d6\", \"#e5bbe5\", \"#f4d6f4\", \"#fdf0fd\"],\n",
    "    \"LEAD-aVR\":  [\"#ea6a71\", \"#f09196\", \"#f7b8ba\", \"#fddedf\", \"#fff5f5\"]\n",
    "}\n",
    "\n",
    "\n",
    "peak_labels = ['P', 'Q', 'R', 'S', 'T']\n",
    "\n",
    "# Function to prepare pie data\n",
    "def prepare_pie_data(lead_data, peak_data):\n",
    "    outer_values = []\n",
    "    outer_colors = []\n",
    "    outer_labels = []\n",
    "    for lead, percent in lead_data.items():\n",
    "        for idx, peak_pct in enumerate(peak_data[lead]):\n",
    "            value = peak_pct * percent / 100.0\n",
    "            outer_values.append(value)\n",
    "            outer_colors.append(lighter_colors[lead][idx])\n",
    "            outer_labels.append(peak_labels[idx])\n",
    "    inner_values = list(lead_data.values())\n",
    "    inner_colors = [base_colors[k] for k in lead_data]\n",
    "    inner_keys = list(lead_data.keys())\n",
    "    return outer_values, outer_colors, outer_labels, inner_values, inner_colors, inner_keys\n",
    "\n",
    "outer_values1, outer_colors1, outer_labels1, inner_values1, inner_colors1, inner_keys1 = prepare_pie_data(lead_data_1, peak_data_1)\n",
    "outer_values2, outer_colors2, outer_labels2, inner_values2, inner_colors2, inner_keys2 = prepare_pie_data(lead_data_2, peak_data_2)\n",
    "\n",
    "# Plotting side by side\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 6))\n",
    "axs[0].text(0.5, 0.95, \"PH Detection\", transform=axs[0].transAxes,\n",
    "            fontsize=18, fontweight='bold', ha='center')\n",
    "axs[1].text(0.5, 0.95, \"Phenotyping PH\", transform=axs[1].transAxes,\n",
    "            fontsize=18, fontweight='bold', ha='center')\n",
    "\n",
    "\n",
    "for ax, outer_values, outer_colors, outer_labels, inner_values, inner_colors, inner_keys in zip(\n",
    "        axs, [outer_values1, outer_values2], [outer_colors1, outer_colors2], [outer_labels1, outer_labels2],\n",
    "        [inner_values1, inner_values2], [inner_colors1, inner_colors2], [inner_keys1, inner_keys2]):\n",
    "\n",
    "    wedges_outer, _ = ax.pie(\n",
    "        outer_values, radius=1, labels=outer_labels, colors=outer_colors,\n",
    "        wedgeprops=dict(width=0.3, edgecolor='white'),\n",
    "        labeldistance=1.01, textprops={'fontsize': 9, 'fontweight': 'bold'}\n",
    "    )\n",
    "\n",
    "    wedges_inner, _ = ax.pie(\n",
    "        inner_values, radius=0.7, labels=None, colors=inner_colors,\n",
    "        wedgeprops=dict(width=0.3, edgecolor='white')\n",
    "    )\n",
    "\n",
    "    for w, val in zip(wedges_outer, outer_values):\n",
    "        ang = (w.theta2 + w.theta1) / 2\n",
    "        x = np.cos(np.deg2rad(ang)) * 0.85\n",
    "        y = np.sin(np.deg2rad(ang)) * 0.85\n",
    "        ax.text(x, y, f\"{val:.1f}%\", ha='center', va='center', fontsize=8)\n",
    "\n",
    "    for w, val, name in zip(wedges_inner, inner_values, inner_keys):\n",
    "        ang = (w.theta2 + w.theta1) / 2\n",
    "        x = np.cos(np.deg2rad(ang)) * 0.5\n",
    "        y = np.sin(np.deg2rad(ang)) * 0.5\n",
    "        ax.text(x, y + 0.01, f\"{name}\", ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "        ax.text(x, y - 0.05, f\"{val:.1f}%\", ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"quantitative_analysis.pdf\", format=\"pdf\", dpi=600, bbox_inches='tight')\n",
    "\n",
    "import fitz \n",
    "# Open PDF\n",
    "doc = fitz.open(\"quantitative_analysis.pdf\")\n",
    "page = doc[0]\n",
    "\n",
    "# Crop the page: shrink the bottom (units = points; 72 points = 1 inch)\n",
    "rect = page.rect\n",
    "new_rect = fitz.Rect(rect.x0+45, rect.y0+15, rect.x1-45, rect.y1- 45)  # crop 30pt from bottom\n",
    "page.set_cropbox(new_rect)\n",
    "\n",
    "# Save\n",
    "doc.save(\"quantitative_analysis_cropped.pdf\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-23T18:05:18.671422200Z",
     "start_time": "2025-08-23T18:05:18.669422100Z"
    }
   },
   "id": "f4ce1b2c140812c5",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
